# GRPO Algorithm Configuration
defaults: "grpo_math_8B_megatron.yaml"

loss_fn:
  use_importance_sampling_correction: true

policy:
  model_name: "Qwen/Qwen3-8B-Base"
  megatron_cfg:
    converter_type: "Qwen3ForCausalLM"
    tensor_model_parallel_size: 4
    pipeline_model_parallel_size: 1
  generation:
    vllm_cfg:
      precision: 'fp8'
      kv_cache_dtype: 'fp8'
      use_deep_gemm: true
      num_last_layers_in_bf16: 0
      num_first_layers_in_bf16: 0
